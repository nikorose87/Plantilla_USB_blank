\cleardoublepage %poner esta linea al inicio de cada capitulo
\chapter{Introduction}
Particularly, due to the fact that in Colombian restaurants not all the sanitary protocols are always followed properly, it is common to get sick due to this issue \citep{florez2011factores}. In this article, the factors related to illnesses transmitted through food in restaurants of 5 Colombian cities are discussed, $300$ establishments and $1522$ food handlers were surveyed, a microbiological hands inspection was carried out in the food handlers, $1286$ of them were subjected to fecal culture examination, finding the following: intestinal parasites in $346$ subjects with $26.9\%$; $49$ subjects with $3.8\%$ were positive for pathogenic parasites, six subjects showed $0.46\%$ pathogenic enterobacteriaceae. Finally, $0.52\%$ were found in six subjects for Staphylococcus aureus. 

The main cause for this sanitary problem is the food manipulation by humans, that can be avoided by the implementation of mobile robots in food services since it conducts autonomous transport activities in the serving work-space. We assume that the less human manipulation of food, the easier to maintain a high level of hygiene. Moreover, restaurant's efficiency might be improved by automating the whole process of serving. In order to perform this task, the robot should contain certain functions, for instance, the perception of its environment and the ability to avoid static and dynamic obstacles, as well as specific tasks such as detecting the table to which it is required to take \--- or deliver \--- the food plate.

%You will need to introduce the usefullness of ARuco markers in the 

%something that can be very useful in a restaurant for transport of food dishes, making easier to maintain a high level of hygiene and even improve the restaurant's efficiency.


\section{Problem statement}

On the one hand, restaurants showed inconveniences in terms of employees health factors. Disregarding the occupational risks workers are exposed, the degeneration of the overall health due to repetitive tasks such as the constant loads during for more than twelve hours per day is a matter of concern.
%
On the other hand, nowadays due to the COVID-19 pandemic problem, the WHO recommended avoiding social interaction in common places as restaurants, where the implementation of mobile robots will contribute to keep this suggestion because of the waiter-client interaction is avoided and, robot materials reduce the risk of virus transmission thanks to the ease of components disinfection.


\section{Justification and research question}

Which tracking system in a mobile robot will be able to perform predefined food serving functions for the delivery dishes in indoor restaurants?

\subsection{Motivation}

The motivation of this project serves to improve the serving efficiency and the restaurant's sanitation. In which, uncontrollable hygiene problems are commonly shown, these problems are due to the waiter's repetitive tasks which require higher focus to keep a healthy environment. Our project solves this problems because the mobile robot can have the same efficiency all day with less sanitation´s problems.   

\section{Objectives}

\subsection{General objective}

Propose a mobile robot that make the delivery service in a restaurant  using ArUco markers. 

\subsection{Specific objectives}
\begin{itemize}
	\item Determine the technical requirements (Hardware and software) for the prototype implementation.
	\item Implement the programmable framework for the movement and vision systems on the mobile robot. 
	\item Verify the mobile robot functionality in real-world implementations.
\end{itemize}


\section{Scope and limitations}
\textbf{Scope:}

The current study will only cover restaurants which have even surfaces and, if those places are composed of different floors, they must to count with their respective ramps.

\textbf{Limitations:}

The mobile robot will work with the optimal operating conditions subjected to our resources. On the other hand, it is not intended to create a mobile robot that performs functions other than the delivery and receipt of food dishes.

\cleardoublepage
\chapter{Preliminaries and related work}


\section{Mobile robots}

The mobile robots are a widely studied field of robotics, they have been explored due to their useful applications, we can find different type of robots like differential drive robots \citep{akhtar2011dynamic}, three wheeled omnidirectional robots \citep{hacene2019fuzzy} or, car-like robots \citep{wang2021training}. These classes performs moving in different ways, for instance, the omnidirectional class is able to move any direction without restrictions, whereas a differential drive and the car-like robot show mechanical limitations for rotating.

There are two main methods for the representation of the robots, the kinematic and the dynamic model. A kinematic model can be simpler than the dynamic model as mentioned by \citep{akhtar2011dynamic}. However, a dynamic model will be more useful if controlling the torques in each wheel is required, as shown in \citep{arcos2019optimal}. Even though the measurement of the current (with sensors) in each motor is needed, which implies more expensive and complicated robot to deal with.  

%Check this paragraph
In the study from \citep{wang2021training}, it was presented a mobile robot platform used for Smart Manufacturing (SM) that integrates a vision-based operation for the tracking and manipulation of objects, a LiDAR (Laser Imaging Detection and Ranging) to sense the surrounding environment, an IMU (Inertial Measurement Unit) to get the pose of the robot, and finally, an ultrasonic sensors for distance measuring between obstacles and the robot. This can be categorized as a robust robot in function of the hardware peripherals and software design, making possible to reach a high autonomy level and, being able to avoid obstacles, reach a target point, detect collisions and lastly, planning the manipulator and platform paths.     

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.5\textwidth]{Figs/Robot_types.png}
    \caption{A variety of commercial mobile robots available.}
    \label{fig:Aruco_bits}
\end{figure}


\subsection{Obstacle avoidance}

A fundamental aspect of the proposed mobile robot is the obstacle avoidance due to its capability to response at the different situations that affects its environment, Moreover, in \citep{azeta2019obstacle} study was implemented the  Arduino Uno micro-controller, a Wi-Fi module, an Arduino motor shield driver to give control, a HC-SR04 ultrasonic sensor, and, geared motors to provide locomotion to the robot. The main programming language implemented was C++. The sensor emits an ultrasonic pulse each $0.3$s to detect any obstruction between $2$cm  and $400$cm. Finally, for the power supply a $7.4$V 2400mAh lithium polymer battery was implemented.

Offenly, It is possible to combine behaviors like wall following and obstacle avoidance \citep{hacene2019fuzzy}, that facilitates a simpler design. The common method to deal with the obstacle avoidance is to define first a safety distance and, in case of sensors detect a distance under that limit, the robot will switch behaviors, from the main to the obstacle avoidance. The definition of its rules are based on which sensors have detected an obstacle \citep{tzou2009high} \citep{hacene2019fuzzy}, this is a suitable option when no artificial intelligence nor complex techniques are used for the avoidance, although this may reduce the efficiency to complete the main task.

Furthermore, in the \citep{arcos2019optimal} study it was showed another possibility to avoid an obstacle. This consists of implementing a time polynomial function to define the obstacle's position. The method was used to satisfy such constraints, computes the euclidean distance between the center of the robot, and the center of the obstacle. That difference should be bigger than a safety distance, determined by the radius of the robot and the obstacle, plus a factor of security.

%Checked by Nikolay Prieto 8/20/2021

\subsection{Speed control}
The next step for the mobile robot was its movement for this we knew that we had to use control techniques to control the robot´s speed this is how we found this document \citep{abril2012analisis} that talks about if it´s necessary implement a control on a line tracker robot, because of this they made a comparison between a line tracker robot with fuzzy control that is a control technique that allows to work with information that is not accurate and the same robot but with a classic control. At the end of the document they conclude that with fuzzy control the mobile robot Robotino follows the desired trajectory receiving controller speeds up over 250m/s until 750m/s, on the other hand with the classic control the mobile robot Robotino can complete the desired trajectory but just with low speeds in addition we find another document that talks about the speed control with a PID controller, in \citep{thanh2019restaurant} they did a restaurant serving robot using double line-sensors following approach in combination with PID controller for stable speed outcome, the mobile robot is programmed to go to a specific table by mapping data and come back when it´s completed the task.Based on the line reading algorithm from two LED array line sensors implemented on micro controller.

\subsection{Location system}
Another thing to take in account its the location system because we need the real position of the robot to calculate the errors for the control that´s why we search how can we get the pose and position of the robot and we found this\citep{yuan2016rgb} where they did a location system with an Extend Kalman Filter(EKF) method to calculate robot position and pose also in \citep{tzou2009high} they made a location system using laser positioning system. The laser positioning system is used for rapid and precise positioning and guidance of the mobile robot. In other matters we found this document \citep{oltean2010position} that talks about a mobile robot that use 3 RFID readers with 13.56 MHz frequency are on the bottom of the robot and detect the RFID tags that are in the range of magnetic field of three RFID readers the RFID readers will obtain the absolute coordinates stored in the RFID tag and that´s how the robot reach the tables.The size of the RFID reader antenna
is 2.2 2.2 × cm and the size of the tag is 2.8 2.8 × cm.




\section{Fiducial markers}



\subsection{ArUco markers}
ArUco markers are very used for pose estimation and to implement automatic follow-up, for example, in the study of \citep{bacik2017autonomous} where ArUco markers are used for the navigation of a quadrocopter to make it follow a desired trajectory without any additional sensor, just using ArUco markers, in addition, they used ROS (Robotic Operating System), the open source software which is used for the mapping algorithms in that case. In \citep{jimenez2018sistema} we can see another application, in this case the cameras are static and the objective is to follow a object in the range vision of the cameras, the ArUco marker is used as a reference point for the pose estimation, which is cooperative, so it's necessary to combine the information of the two cameras to get better results. The fact that the cameras are statics is a problem when the object moves far away from the range vision, it's really important to have that in consideration as well as the issues that can generate the lighting changes in the environment at the moment of the detection. 

The resolution of the cameras didn't mean a significant change in the results of the detection as is concluded in \citep{kalaitzakis2021fiducial}, while things like contrast, white balance and images produced with smoother edges can decrease the accuracy and precision of the detection. ArUco had similar results in therms of detection rate compared with other fiducial markers like STag, AprilTag and ARTag, requires less hardware resources, so it’s implementation can be cheaper, it had really good results but it’s sensitive to small markers size and larger distances as is mentioned too in \citep{ferrao2018detection}.     

The implementation of the mobile robots affect positively the efficiency of a restaurant and also mitigate some charges that would have a human. We can see in this surveys the problems that present a normal restaurant.


In the study of \citep{florez2011factores} you can see surveys conducted about sanitation problems in some restaurants, some of those problems are simultaneous handling of money and food (17\%), wearing jewlery (15,2\%),Long fingernails (8,9\%) inter alia. On the other hand in \citep{armendariz2012riesgos} they talk about the health problems of workers also they carried out surveys with these results:

\begin{itemize}
\item 100\% of the waiters expressed high level concentration and physical efforts.
\item 87\% expressed forced work postures.
 
\item 80\% expressed that heavy handling is a risk in their professions.

\end{itemize}


\section{ROS} 
 ROS is a middleware that allows to control robots and make simulations quite close to the reality, it have been used most frequently in the recent years but it can be explored more, in \citep{osio2018desenvolupament} and \citep{zhang2020robot} are presented applications to real mobile robots through ROS where is remarkable the possibility of send commands to the robot using the local network and the easy integration of the computer vision with ROS. Another field of robotics where the use of ROS can be found is in the control of industrial arms as is presented in \citep{arents2018integration} where they integrate computer vision, artificial intelligence and the software Move It!, and as is mentioned there are ROS libraries, drivers and tools created for interfacing with industrial robots that prove the importance of ROS and how it can make the robot programming an easier task.
 

Finally, in the Table \ref{tab:related_work_direct} you related applications that inspired our project:


\begin{table}[h!]
\caption{Direct applications}\label{tab:related_work_direct}
\begin{centering}
\begin{tabular}{>{\centering}p{7cm}>{\centering}p{4cm}}
\hline 
\raggedright{}Description & \raggedright{}Reference\tabularnewline
\hline 
\raggedright{Implemetation of a waiter robot on a Pioneer 3DX robot} & \raggedright{}\citep{escudero2012robot}\tabularnewline
\hline 
\raggedright{}Mapping and location with a depth camera and movement base with gyroscope and odometry & \raggedright{}\citep{zhang2016approach}\tabularnewline
\hline 
\raggedright{}PID controller for stable speed and double-line sensor& \raggedright{}\citep{thanh2019restaurant}\tabularnewline
\hline 
\raggedright{}Reinforcement Q-learning PID Controller for a Restaurant Mobile Robot with Double Line-Sensors & \raggedright{}\citep{thanh2020reinforcement}\tabularnewline
\hline 
\raggedright{}Fuzzy Logic Based Control for Autonomous Mobile Robot Navigation& \raggedright{}\citep{omrane2016fuzzy}\tabularnewline
\hline 
\raggedright{}2 types of robots one to take the order and another to recive the dishes & \raggedright{}\citep{eksiri2015restaurant}\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\end{table}


\begin{center}
\begin{table}[h!]
\caption{Indirect applications}\label{tab:related_work_indirect}
\centering{}%
\begin{tabular}{>{\raggedright}p{4cm}>{\raggedright}p{4cm}>{\raggedright}p{4cm}}
\hline 
\raggedright{}Topic & \raggedright{}Description & \raggedright{}Reference\tabularnewline
\hline 
\raggedright{}Obstacle avoidance  & \raggedright{}Obstacle detection using ultrasonic sensor for a mobile robot & \raggedright{}\citep{azeta2019obstacle}\tabularnewline
\hline 
\raggedright{}Speed control & \raggedright{}Implementation of a robust speed control with \textit{fuzzy control} technique
through Labview & \raggedright{}\citep{abril2012analisis}\tabularnewline
\hline 
\raggedright{}Location system & \raggedright{}High speed laser location & \raggedright{}\citep{tzou2009high}\tabularnewline
\hline 
\raggedright{}Positioning system & \raggedright{}Positioning system using radio frequency identification system (RFID)  & \raggedright{}\citep{qing2010research}\tabularnewline
\hline 
\raggedright{}Location system & \raggedright{}Location based on RGB and depth RGB (RGD-D) to avoid obstacles & \raggedright{}\citep{yuan2016rgb}\tabularnewline
\hline 
\raggedright{}Path planning with dynamic obstacle avoidance & \raggedright{}From dynamic models they define a follow-up strategy with non lineal trajectories  & \raggedright{}\citep{arcos2019optimal}\tabularnewline
\hline 
\end{tabular}
\end{table}
\par\end{center}





\cleardoublepage
\chapter{Conceptual frame/Main part}
\section{Estimation of the ArUco POSE}
\subsection{ArUco markers}
ArUco is a really useful open source library for marker detection through computer vision techniques, it contains some dictionaries with a large number of markers, where each marker can be identified with an unique id obtained from the recognition of a binary pattern in the markers as is shown in \autoref{fig:Aruco_bits} this patterns depends on the amount of bits according to the selected dictionary \citep{salinas2019aruco}.

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.4\textwidth]{Figs/ArUco24.png}
    \caption{ArUco marker from the original ArUco dictionary, which have a 5x5 bits dimensions}
    \label{fig:Aruco_bits}
\end{figure}

The wide number of marker id's and the accuracy in the detection of the Aruco markers and the fact that the library is available for Python makes it really appropriated to be used in this thesis work for the detection of each table. In the \autoref{fig:Aruco_detect}  we can see an example of the ArUco marker detection with Python and the Opencv library. \autoref{eq:Distance}

\begin{figure}[H]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.6\textwidth]{Figs/Aruco.JPG}
    \caption{Marker position estimation}
    \label{fig:Aruco_detect}
\end{figure}

\subsection{Camera calibration}

The coordinates shown in \autoref{fig:Aruco_detect}, represents the estimated position of the marker respect to a frame located in the center of the camera, to get good results in the estimation is necessary to obtain the calibration and distortion matrix of the camera, there are two main ways to make the calibration process, the first one is with the ArUco calibration board, and the second one is with the classical Opencv chessboard which is the easier one and the selected in this case, in the \autoref{fig:Chessboard} we can see a set of pictures took with the camera, then this pictures are loaded to a Python program that select the good pictures and outputs the calibration and distortion matrix.

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.6\textwidth]{Figs/Calib.png}
    \caption{Images for the camera calibration process}
    \label{fig:Chessboard}
\end{figure}



\section{Control design}
\subsection{Motor speed control}
In order to control the motor, first we needed to obtain the motor parameters for modeling and design the appropriate controller, we used the experimental methods introduced in {} and got the following values:
\[b=0.1376\ N-m-s\]
\[R_a=2.2\ \Omega\]
\[L=0.1834\ H\]
\[K_e=K_t=0.5115\frac{V}{rad/s}\]
\[J=0.0099\ Kg\ast m^2\]
Dynamic equations of the system:
\[J\ddot{\theta}+b\dot{\theta}=Ki\ \]
\[L\frac{di}{dt}+Ri=V-K\dot{\theta}\ \ \] 
\subsubsection{State-space representation:}
First, let's define the state variables as : $\theta, \dot{\theta},i$, then, the state equations can be written as:
\[\frac{d\theta}{dt}=\dot{\theta}\]
\[\frac{d\dot{\theta}}{dt}=-\frac{b}{J}\dot{\theta}+\frac{K}{J}i\]
\[\frac{di}{dt}=-\frac{K}{L}\dot{\theta}-\frac{R}{L}i+\frac{V}{L}\]

So, finally we have:
\begin{equation}
    \frac{d}{dt}\left[\begin{matrix}\theta\\\dot{\theta}\\i\\\end{matrix}\right]=\left[\begin{matrix}0&\ \ \ \ 1&\ \ \ \ \ \ 0\\0&-\frac{b}{J}&\ \ \ \ \ \frac{K}{J}\\0&-\frac{K}{L}&\ \ \ \ \ \frac{R}{L}\\\end{matrix}\right]\left[\begin{matrix}\theta\\\dot{\theta}\\i\\\end{matrix}\right]+\left[\begin{matrix}0\\0\\\frac{1}{L}\\\end{matrix}\right]V
    \label{eq:State_space_3}
\end{equation}
\[y=[1\ 0\ 0]\left[\begin{matrix}\theta\\\dot{\theta}\\i\\\end{matrix}\right]\]

\subsubsection{State-space control}
The previous system is not C.C (Completely controllable) so we have to ignore the first state to deal with that problem and be able to design the controller:
\[\frac{d}{dt}\left[\begin{matrix}\dot{\theta}\\i\\\end{matrix}\right]=\left[\begin{matrix}-\frac{b}{J}&\ \frac{K}{J}\\-\frac{K}{L}&\frac{R}{L}\\\end{matrix}\right]\left[\begin{matrix}\dot{\theta}\\i\\\end{matrix}\right]+\left[\begin{matrix}0\\\frac{1}{L}\\\end{matrix}\right]V\]
\[y=[1\ 0]\left[\begin{matrix}\dot{\theta}\\i\\\end{matrix}\right]\]
We discrete the system, and define the controller, which it's going to be designed to track step reference, so the controller is defined in the state-space form as:
\[x_c (k+1)=[1] x_c  (k)+[1]e(k)\]
Using Matlab we find the gains for the control with $t_s=0.4 s$ as setting time, obtaining:

\[k_c=12.2503\]
\[k_p=[20.1927\ \ \ 48.4480]\]

\subsubsection{State-space observer}

Now, as we only have direct measurement of the position through encoders, we must design an observer for the estimation of the velocity and the current in order to complete the full state feedback control scheme, in this case we need the \ref{eq:State_space_3} representation:

We make a copy of the system:
\[\hat{x}\left(k+1\right)=A\hat{x}+L(y-C\hat{x})\ \]
\[\hat{x}\left(k+1\right)=(A-LC)\hat{x}+Ly\]
Using Matlab, we find the values for the L vector with and obtain the following observer matrices:
\[\left[\begin{matrix}\theta(k+1)\\\dot{\theta}(k+1)\\i(k+1)\\\end{matrix}\right]=\left[\begin{matrix}-0.4178&\ \ \ \ 0.0342&0.0413\\-4.7212&0.4096&1.2719\\1.0504&-0.0688&0.4560\\\end{matrix}\right]\left[\begin{matrix}\theta(k)\\\dot{\theta}(k)\\i(k)\\\end{matrix}\right]+\left[\begin{matrix}0.0003&1.4178\\0.0173&4.7212\\0.0150&-1.0504\\\end{matrix}\right]\left[\begin{matrix}u(k)\\y(k)\\\end{matrix}\right]\]

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.9\textwidth]{Figs/Control_motor.pdf}
    \caption{Control scheme with Luenberger's observer}
    \label{fig:Speed_Control_scheme}
\end{figure}

In the next figure is shown the response of the system in closed loop with the feedback of the estimated states

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.6\textwidth]{Figs/Control_motor.eps}
    \caption{Control response}
    \label{fig:Speed_Control_response}
\end{figure}




\newpage
\subsection{Differential drive robot}
A differential drive robot consists in a robot with two motors attached to each wheel, this make possible to turn the robot but it can't move in any direction at any moment, it have to turn first. The robot can be represented in a kinematic form as a point moving in the space, this is known as the unicycle model, which translational and angular velocities are defined by:
\begin{equation}
    \dot{x}=v*cos(\theta)
\label{eq:X_dot}
\end{equation}
\begin{equation}
    \dot{y}=v*sin(\theta)
\label{eq:Y_dot}
\end{equation}
\begin{equation}
    \dot{\theta}=\omega
\label{eq:Omega}
\end{equation}


Where $v$ and $\omega$ are derivated from the followings equations, R is the radius of the wheels, L is the separation between the wheels, $v_{r}$ and $v_{l}$ are the velocities of each wheel in rad/s:
\[v=\frac{R}{2}*(v_r+v_l)\]
\[\omega=\frac{R}{L}*(v_r-v_l)\]
Replacing $v$ and $\omega$ in \autoref{eq:X_dot}, \autoref{eq:Y_dot} and \autoref{eq:Omega}:
\[\dot{x}=\frac{R}{2}*(v_r+v_l)*cos(\theta)\]
\[\dot{y}=\frac{R}{2}*(v_r+v_l)*sin(\theta)\]
\[\dot{\phi}=\frac{R}{L}*(v_r-v_l)\]





\subsection{Go-to-goal behavior}

First we need to define the errors of position and orientation from the robot to the goal:

\[e_x=X_d-X\]
\[e_y=Y_d-Y\]
\[e_{heading}=\theta_d-\theta\]


\[V=K_{pv}*\sqrt{e_x^2+e_y^2}\]
\[\omega=K_{p\omega}*\arctan{\frac{\sin{e_{heading}}}{\cos{e_{heading}}}}\]




\subsection{Obstacle avoidance}


\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.6\textwidth]{Figs/Flowchart.eps}
    \caption{Fluid process diagram}
    \label{fig:Chessboard}
\end{figure}

\section{ROS and Gazebo simulation}
\subsection{ROS}
The Robot Operating System (ROS) is an open-source framework that helps researchers and developers build and reuse code between robotics applications. ROS is also a global open-source community of engineers, developers and hobbyists who contribute to making robots better, more accessible and available to everyone.
\subsubsection{Topics}
Topics are named buses over which nodes exchange messages. Topics have anonymous publish/subscribe semantics, which decouples the production of information from its consumption. In general, nodes are not aware of who they are communicating with. Instead, nodes that are interested in data subscribe to the relevant topic; nodes that generate data publish to the relevant topic. There can be multiple publishers and subscribers to a topic.
\subsubsection{Nodes}
A node is a process that performs computation. Nodes are combined together into a graph and communicate with one another using streaming topics, RPC services, and the Parameter Server. These nodes are meant to operate at a fine-grained scale; a robot control system will usually comprise many nodes. For example, one node controls a laser range-finder, one Node controls the robot's wheel motors, one node performs localization, one node performs path planning, one node provides a graphical view of the system, and so on.
\subsection{Gazebo}
\subsection{Simulated environment}
\subsubsection{URDF}
An URDF model is a XML-based representation of the physics and the visual properties of the robot and the other world elements, this model is necessary for the simulation 
\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{Figs/Gzworld.jpg}
    \caption{Simulated tables and it's ArUco markers attached to each one in a Gazebo world}
    \label{fig:Gzworld}
\end{figure}
\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{Figs/Cords_tables.png}
    \caption{Coordinates of the tables in the simulated environment}
    \label{fig:Cords_Tables}
\end{figure}

In the simulated world the robot follows a predefined trajectory when no ArUco marker is detected, the robot navigates through the simulated restaurant trying to find the requested ArUco marker and when it finally detects it switch to a go to the table behavior.

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{Figs/Trajectory1.png}
    \caption{Predefined trajectory}
    \label{fig:Cords_Tables}
\end{figure}


\newpage
\section{Prototype design}
The hardware design of our mobile robot was based on Turtlebot3 Burger  it´s a mobile robot designed by Robotics company, The Turtlebot3 Burger its a modular and customisable mobile robot which components are a RaspberryPi 3, Laser Distance sensor, OpenCR and Dynamixel X.

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.3\textwidth]{Figs/Burger.png}
    \caption{TurtleBot 3 Burger}
    \label{fig:Burger}
\end{figure}

Our mobile robot was designed on Autodesk Inventor software and then 3D printed, the base material is PLA that is a plant-based, biodegradable plastic and the material of the columns is Aluminum.

The parts of our design are 3 bases supported by 4 columns each one, the first has 15 cm radius and 1 cm thick, the second base has 15 cm radius and 0.5 cm thick and the second has 10 cm radius and 0.5 cm thick, the columns have 5mm radius and 30cm in length, the robot´s hardware are as follows. The Raspberry Pi, Li-po Batteries, H-bridges and ultrasonic sensors are on the first base and the camera is on the top of the mobile robot in addition the motors and the encoder are bracket mounted on the bottom of the base.    

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.45\textwidth]{Figs/Robot.png}
    \caption{Mobile Robot design on Autodesk Inventor}
    \label{fig:Robot}
\end{figure}

\newpage
\section{Implementation}

\subsection{Raspberry Pi 3}
The Raspberry Pi is a single-board computer commonly used in robotics and IoT applications, due to the fact that ROS can be run in the Raspberry, makes it appropriated to be selected as the main component of the project, here is where the ROS topics with the information of the sensor will be published, and through it the commands to move the robot will be sent.

\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.3\textwidth]{Figs/Raspberry1.png}
    \caption{Raspberry Pi Model B+}
    \label{fig:RPI}
\end{figure}

\begin{figure}[H]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{Figs/RPI.png}
    \caption{Raspberry Pi GPIO ports connection}
    \label{fig:RPI_ports}
\end{figure}

\begin{table}[H]
\caption{Raspberry Pi B+ specifications}
\begin{centering}
\begin{tabular}{>{\centering}p{7cm}>{\centering}p{4cm}}
\hline 
\raggedright{}Power supply & \raggedright{}5V/2.5A DC\tabularnewline
\hline 
\raggedright{}SoC & \raggedright{}Broadcom BCM2837\tabularnewline
\hline 
\raggedright{}CPU & \raggedright{}1.4GHz 64-bit quad-core ARMv8\tabularnewline
\hline 
\raggedright{}USB ports & \raggedright{}4\tabularnewline
\hline 
\raggedright{}GPIO ports & \raggedright{}40\tabularnewline
\hline 
\raggedright{}RAM & \raggedright{}1 GB\tabularnewline
\hline 
\raggedright{}Disk & \raggedright{}8 GB SD card\tabularnewline
\hline 
\raggedright{}OS & \raggedright{}Raspbian\tabularnewline
\hline 
\raggedright{}ROS distro & \raggedright{}Melodic\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\end{table}

\subsection{Ultrasonic sensor HC-SR04}
This sensor is used for distance measurement, the sensor have a transmitter and a receiver as is shown in  \autoref{fig:Sonar}, basically it generates a wave sound and then listen to the echo produced when the wave bounces in the object in front of the sensor, the time that the wave takes to go back to the sensor and the sound speed are used to calculate the distance between the sensor and the obstacle with the following equation \citep{morgan2014hcsr04}: 
\begin{equation}
    distance=speed*time
\label{eq:Distance}
\end{equation}
The trigger pin of the sensor receives is used to send a 10 nano seconds ultrasonic pulse, the echo pin sets to high while the pulse is being received, the time that echo pin is set to high ($t$), the distance traveled to the object and the distance traveled back to the sensor ($2d$) and the sound speed $(340 m/s)$ are replaced in \autoref{eq:Distance}, obtaining:
\[2d=340m/s*t\]
\[d=170m/s*t\]
\begin{figure}[h!]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.8\textwidth]{Figs/Sonar1.png}
    \caption{Sonar sensor}
    \label{fig:Sonar}
\end{figure}

\begin{table}[h!]
\caption{Hc-Sr04 specifications}
\begin{centering}
\begin{tabular}{>{\centering}p{7cm}>{\centering}p{4cm}}
\hline 
\raggedright{}Power supply & \raggedright{}+5V DC\tabularnewline
\hline 
\raggedright{}Effectual angle & \raggedright{}<15°\tabularnewline
\hline 
\raggedright{}Ranging distance & \raggedright{}2-400 cm\tabularnewline
\hline 
\raggedright{}Resolution & \raggedright{}0.3 cm\tabularnewline
\hline 
\raggedright{}Measuring angle & \raggedright{}30°\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\end{table}



\subsection{H-Bridge BTS-7960}

The H-Bridge BTS-7960 alloys to control the velocity of each motor, it’s able to support high current levels, what makes it ideal for control high-torque motors.
\begin{table}[H]
\caption{H-Bridge Bts7960 specifications}
\begin{centering}
\begin{tabular}{>{\centering}p{7cm}>{\centering}p{4cm}}
\hline 
\raggedright{}Power supply & \raggedright{}5.5-27 V\tabularnewline
\hline 
\raggedright{}Logic input voltage & \raggedright{}-0.3-5 V\tabularnewline
\hline 
\raggedright{}Max frequency operation & \raggedright{}25 KHz\tabularnewline
\hline 
\raggedright{}Max current supported & \raggedright{}43 A\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\end{table}


\begin{figure}[H]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.6\textwidth]{Figs/Hbridge1.png}
    \caption{H-Bridge}
    \label{fig:H-bridge}
\end{figure}

\subsection{Generic Raspberry Pi camera PI REV 1.3}
The generic raspberry pi camera have a good resolution and it’s price is very low compared to other cameras with similar properties. It can be connected directly to the Raspberry Pi through a CSI interface.
\begin{figure}[H]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.2\textwidth]{Figs/Rasp_camera.png}
    \caption{Raspberry Pi camera PI REV 1.3}
    \label{fig:Rasp_camera}
\end{figure}

\begin{table}[H]
\caption{PI REV 1.3 specifications}
\begin{centering}
\begin{tabular}{>{\centering}p{7cm}>{\centering}p{4cm}}
\hline 
\raggedright{}Resolution & \raggedright{}5 MP\tabularnewline
\hline 
\raggedright{}Maximum static images size & \raggedright{}2592x1944\tabularnewline
\hline 
\raggedright{}Video formats & \raggedright{}1080p30, 720p60 and 640x480p60/90\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\end{table}

\subsection{Motors, encoders and batteries}

The selected motor is a JGA25-370 DC motor with a torque of 10 Kg-cm what makes it capable to move the robot even when it have a plate over it, this motor have attached to it a hall effect encoder which will be used for the measurement of the position and the velocity of the robot and the motors.  

\begin{figure}[H]
    \centering
 	\includegraphics[trim=0 0 0 0,clip,width=0.4\textwidth]{Figs/Batteries1.png}
    \caption{18650 Batteries}
    \label{fig:Batteries}
\end{figure}

\begin{table}[H]
\caption{Parts specifications}
\begin{centering}
\begin{tabular}{>{\centering}p{7cm}>{\centering}p{4cm}}
\hline 
\raggedright{}Batteries & \raggedright{}3,7V, 2200 mAh\tabularnewline
\hline 
\raggedright{}Encoders resolution& \raggedright{}748 PPR\tabularnewline
\hline 
\raggedright{}Encoders supply voltage & \raggedright{}3-3,3V\tabularnewline
\hline 
\raggedright{}Motors supply voltage & \raggedright{}6V\tabularnewline
\hline 
\raggedright{}Velocity & \raggedright{}100 RPM\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\end{table}

\newpage

\cleardoublepage
\chapter{Conclusion}





